{"cells":[{"metadata":{},"cell_type":"markdown","source":"# COVID-19: Topic Modelling and Search with Top2Vec\n\n[Top2Vec](https://github.com/ddangelov/Top2Vec) is an algorithm for **topic modelling** and **semantic search**. It **automatically** detects topics present in text and generates jointly embedded topic, document and word vectors. Once you train the Top2Vec model you can:\n* Get number of detected topics.\n* Get topics.\n* Search topics by keywords.\n* Search documents by topic.\n* Find similar words.\n* Find similar documents.\n\nThis notebook preprocesses the [Kaggle COVID-19 Dataset](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge), it treats each section of every paper as a distinct document. A Top2Vec model is trained on those documents. \n\nOnce the model is trained you can do **semantic** search for documents by topic, searching for documents with keywords, searching for topics with keywords, and for finding similar words. These methods all leverage the joint topic, document, word embeddings distances, which represent semantic similarity. \n\n### For an interactive version of this notebook with search widgets check out my [github](https://github.com/ddangelov/Top2Vec/blob/master/notebooks/CORD-19_top2vec.ipynb) or my [kaggle](https://www.kaggle.com/dangelov/covid-19-top2vec-interactive-search)!\n\n"},{"metadata":{},"cell_type":"markdown","source":"# Import and Setup "},{"metadata":{},"cell_type":"markdown","source":"### 1. Install the [Top2Vec](https://github.com/ddangelov/Top2Vec) library"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"!pip install top2vec","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Import Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport json\nimport os\nimport ipywidgets as widgets\nfrom IPython.display import clear_output, display\nfrom top2vec import Top2Vec","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Pre-process Data"},{"metadata":{},"cell_type":"markdown","source":"### 1. Import Metadata"},{"metadata":{"trusted":true},"cell_type":"code","source":"metadata_df = pd.read_csv(\"../input/CORD-19-research-challenge/metadata.csv\")\nmetadata_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Pre-process Papers\n\nA document will be created for each section of every paper. This document will contain the id, title, abstract, and setion of the paper. It will also contain the text of that section."},{"metadata":{"trusted":true},"cell_type":"code","source":"def preproccess_papers():\n\n    dataset_dir = \"../input/CORD-19-research-challenge/\"\n    comm_dir = dataset_dir+\"comm_use_subset/comm_use_subset/\"\n    noncomm_dir = dataset_dir+\"noncomm_use_subset/noncomm_use_subset/\"\n    custom_dir = dataset_dir+\"custom_license/custom_license/\"\n    biorxiv_dir = dataset_dir+\"biorxiv_medrxiv/biorxiv_medrxiv/\"\n    directories_to_process = [comm_dir,noncomm_dir, custom_dir, biorxiv_dir]\n\n    papers_with_text = list(metadata_df[metadata_df.has_full_text==True].sha)\n\n    paper_ids = []\n    titles = []\n    abstracts = []\n    sections = []\n    body_texts = []\n\n    for directory in directories_to_process:\n\n        filenames = os.listdir(directory)\n\n        for filename in filenames:\n\n          file = json.load(open(directory+filename, 'rb'))\n\n          #check if file contains text\n          if file[\"paper_id\"] in papers_with_text:\n\n            section = []\n            text = []\n\n            for bod in file[\"body_text\"]:\n              section.append(bod[\"section\"])\n              text.append(bod[\"text\"])\n\n            res_df = pd.DataFrame({\"section\":section, \"text\":text}).groupby(\"section\")[\"text\"].apply(' '.join).reset_index()\n\n            for index, row in res_df.iterrows():\n\n              # metadata\n              paper_ids.append(file[\"paper_id\"])\n\n              if(len(file[\"abstract\"])):\n                abstracts.append(file[\"abstract\"][0][\"text\"])\n              else:\n                abstracts.append(\"\")\n\n              titles.append(file[\"metadata\"][\"title\"])\n\n              # add section and text\n              sections.append(row.section)\n              body_texts.append(row.text)\n\n    return pd.DataFrame({\"id\":paper_ids, \"title\": titles, \"abstract\": abstracts, \"section\": sections, \"text\": body_texts})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# papers_df = preproccess_papers()\n# papers_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Filter Short Sections"},{"metadata":{"trusted":true},"cell_type":"code","source":"def filter_short(papers_df):\n    papers_df[\"token_counts\"] = papers_df[\"text\"].str.split().map(len)\n    papers_df = papers_df[papers_df.token_counts>200].reset_index(drop=True)\n    papers_df.drop('token_counts', axis=1, inplace=True)\n    \n    return papers_df\n   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# papers_df = filter_short(papers_df)\n# papers_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train Top2Vec Model\nParameters:\n  * ``documents``: Input corpus, should be a list of strings.\n  \n  * ``speed``: This parameter will determine how fast the model takes to train. \n    The 'fast-learn' option is the fastest and will generate the lowest quality\n    vectors. The 'learn' option will learn better quality vectors but take a longer\n    time to train. The 'deep-learn' option will learn the best quality vectors but \n    will take significant time to train.  \n    \n  * ``workers``: The amount of worker threads to be used in training the model. Larger\n    amount will lead to faster training.\n    \nSee [Documentation](https://top2vec.readthedocs.io/en/latest/README.html)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# top2vec = Top2Vec(documents=papers_df.text, speed=\"learn\", workers=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## (Recommended) Load Pre-trained Model and Pre-processed Data :)\n\nThe Top2Vec model was trained with the 'deep-learn' speed parameter and took very long to train. It will give much better results than training with 'fast-learn' or 'learn'.\n\nData is available on my [kaggle](https://www.kaggle.com/dangelov/covid19top2vec)."},{"metadata":{},"cell_type":"markdown","source":"### 1. Load pre-trained Top2Vec model "},{"metadata":{"trusted":true},"cell_type":"code","source":"top2vec = Top2Vec.load(\"../input/covid19top2vec/covid19_deep_learn_top2vec\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Load pre-processed papers"},{"metadata":{"trusted":true},"cell_type":"code","source":"papers_df = pd.read_feather(\"../input/covid19top2vec/covid19_papers_processed.feather\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Use Top2Vec for Semantic Search"},{"metadata":{},"cell_type":"markdown","source":"## 1. Search Topics "},{"metadata":{"trusted":true},"cell_type":"code","source":"keywords_select_st = widgets.Label('Enter keywords seperated by space: ')\ndisplay(keywords_select_st)\n\nkeywords_input_st = widgets.Text()\ndisplay(keywords_input_st)\n\nkeywords_neg_select_st = widgets.Label('Enter negative keywords seperated by space: ')\ndisplay(keywords_neg_select_st)\n\nkeywords_neg_input_st = widgets.Text()\ndisplay(keywords_neg_input_st)\n\ndoc_num_select_st = widgets.Label('Choose number of topics: ')\ndisplay(doc_num_select_st)\n\ndoc_num_input_st = widgets.Text(value='5')\ndisplay(doc_num_input_st)\n\ndef display_similar_topics(*args):\n    \n    clear_output()\n    display(keywords_select_st)\n    display(keywords_input_st)\n    display(keywords_neg_select_st)\n    display(keywords_neg_input_st)\n    display(doc_num_select_st)\n    display(doc_num_input_st)\n    display(keyword_btn_st)\n    \n    try:\n        topic_words, word_scores, topic_scores, topic_nums = top2vec.search_topics(keywords=keywords_input_st.value.split(),num_topics=int(doc_num_input_st.value), keywords_neg=keywords_neg_input_st.value.split())\n        for topic in topic_nums:\n            top2vec.generate_topic_wordcloud(topic, background_color=\"black\")\n        \n    except Exception as e:\n        print(e)\n        \nkeyword_btn_st = widgets.Button(description=\"show topics\")\ndisplay(keyword_btn_st)\nkeyword_btn_st.on_click(display_similar_topics)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Search Papers by Topic"},{"metadata":{"trusted":true},"cell_type":"code","source":"topic_num_select = widgets.Label('Select topic number: ')\ndisplay(topic_num_select)\n\ntopic_input = widgets.Text()\ndisplay(topic_input)\n\ndoc_num_select = widgets.Label('Choose number of documents: ')\ndisplay(doc_num_select)\n\ndoc_num_input = widgets.Text(value='10')\ndisplay(doc_num_input)\n\ndef display_topics(*args):\n    \n    clear_output()\n    display(topic_num_select)\n    display(topic_input)\n    display(doc_num_select)\n    display(doc_num_input)\n    display(topic_btn)\n\n    documents, document_scores, document_nums = top2vec.search_documents_by_topic(topic_num=int(topic_input.value), num_docs=int(doc_num_input.value))\n    \n    result_df = papers_df.loc[document_nums]\n    result_df[\"document_scores\"] = document_scores\n    \n    for index,row in result_df.iterrows():\n        print(f\"Document: {index}, Score: {row.document_scores}\")\n        print(f\"Section: {row.section}\")\n        print(f\"Title: {row.title}\")\n        print(\"-----------\")\n        print(row.text)\n        print(\"-----------\")\n        print()\n\ntopic_btn = widgets.Button(description=\"show documents\")\ndisplay(topic_btn)\ntopic_btn.on_click(display_topics)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Search Papers by Keywords"},{"metadata":{"trusted":true},"cell_type":"code","source":"keywords_select_kw = widgets.Label('Enter keywords seperated by space: ')\ndisplay(keywords_select_kw)\n\nkeywords_input_kw = widgets.Text()\ndisplay(keywords_input_kw)\n\nkeywords_neg_select_kw = widgets.Label('Enter negative keywords seperated by space: ')\ndisplay(keywords_neg_select_kw)\n\nkeywords_neg_input_kw = widgets.Text()\ndisplay(keywords_neg_input_kw)\n\ndoc_num_select_kw = widgets.Label('Choose number of documents: ')\ndisplay(doc_num_select_kw)\n\ndoc_num_input_kw = widgets.Text(value='10')\ndisplay(doc_num_input_kw)\n\ndef display_keywords(*args):\n    \n    clear_output()\n    display(keywords_select_kw)\n    display(keywords_input_kw)\n    display(keywords_neg_select_kw)\n    display(keywords_neg_input_kw)\n    display(doc_num_select_kw)\n    display(doc_num_input_kw)\n    display(keyword_btn_kw)\n    \n    try:\n        documents, document_scores, document_nums = top2vec.search_documents_by_keyword(keywords=keywords_input_kw.value.split(), num_docs=int(doc_num_input_kw.value), keywords_neg=keywords_neg_input_kw.value.split())\n        result_df = papers_df.loc[document_nums]\n        result_df[\"document_scores\"] = document_scores\n\n        for index,row in result_df.iterrows():\n            print(f\"Document: {index}, Score: {row.document_scores}\")\n            print(f\"Section: {row.section}\")\n            print(f\"Title: {row.title}\")\n            print(\"-----------\")\n            print(row.text)\n            print(\"-----------\")\n            print()\n           \n    except Exception as e:\n        print(e)\n        \n\nkeyword_btn_kw = widgets.Button(description=\"show documents\")\ndisplay(keyword_btn_kw)\nkeyword_btn_kw.on_click(display_keywords)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Find Similar Words"},{"metadata":{"trusted":true},"cell_type":"code","source":"keywords_select_sw = widgets.Label('Enter keywords seperated by space: ')\ndisplay(keywords_select_sw)\n\nkeywords_input_sw = widgets.Text()\ndisplay(keywords_input_sw)\n\nkeywords_neg_select_sw = widgets.Label('Enter negative keywords seperated by space: ')\ndisplay(keywords_neg_select_sw)\n\nkeywords_neg_input_sw = widgets.Text()\ndisplay(keywords_neg_input_sw)\n\n\ndoc_num_select_sw = widgets.Label('Choose number of words: ')\ndisplay(doc_num_select_sw)\n\ndoc_num_input_sw = widgets.Text(value='20')\ndisplay(doc_num_input_sw)\n\ndef display_similar_words(*args):\n    \n    clear_output()\n    display(keywords_select_sw)\n    display(keywords_input_sw)\n    display(keywords_neg_select_sw)\n    display(keywords_neg_input_sw)\n    display(doc_num_select_sw)\n    display(doc_num_input_sw)\n    display(sim_word_btn_sw)\n    \n    try:            \n        words, word_scores = top2vec.similar_words(keywords=keywords_input_sw.value.split(), keywords_neg=keywords_neg_input_sw.value.split(), num_words=int(doc_num_input_sw.value))\n        for word, score in zip(words, word_scores):\n            print(f\"{word} {score}\")\n   \n    except Exception as e:\n        print(e)\n        \nsim_word_btn_sw = widgets.Button(description=\"show similar words\")\ndisplay(sim_word_btn_sw)\nsim_word_btn_sw.on_click(display_similar_words)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}